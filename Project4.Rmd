---
title: "Final"
author: "qiankang wang"
date: "20/04/2022"
output: pdf_document
---

# Question 1

## i)

```{r}
knitr::opts_chunk$set(fig.width=6, fig.height=4) 
D = read.csv("~/Desktop/Courses/631/ExamDataEpi.csv")
set.seed(2022)
# Pit
Pit = function(a, b, dists) {
  return(1 - exp(-a * exp(dists*(-b))))
}
idx_1 = D[D["inftime"] == 1][1]
idx_S2 = D["inftime"] == 0
idx_I2 = D["inftime"] == 2
dists = sqrt((sweep(D["x"], 1, D[idx_1, ][["x"]]))^2 + (sweep(D["y"], 1, D[idx_1, ][["y"]]))^2)

# Likelihood
f = function(a, b) {
  return(prod(1 - Pit(a, b, dists[idx_S2])) * prod(Pit(a, b, dists[idx_I2])))
}

# Log Likelihood
log_f = function(a, b) {
  return(sum(log(1 - Pit(a, b, dists[idx_S2]))) + sum(log(prod(Pit(a, b, dists[idx_I2])))))
}

```

## ii)

```{r}
# Compute MLE results
MLE = function(fun) {
  fn = function(x) {
    return(-fun(a = x[1], b = x[2]))
  }
  ret = optim(c(1.0, 1.0), fn, method = "L-BFGS-B", lower = c(0.01, 0.01), upper = c(5, 5))
  return(ret$par)
}

MLE_ret = MLE(log_f)
```

```{r}
# Metropolis Annealing 
annealing = function(fun) {
  alpha = 2
  beta = 3
  moving = 1
  rho = 0.9
  t = 1
  while (moving > 0) {
    moving = 0
    for (i in 1: 1500) {
      while (TRUE) {
        alpha_star = runif(1, alpha - 0.2, alpha + 0.2)
        beta_star = runif(1, beta - 0.2, beta + 0.2)
        if (alpha_star > 0 & beta_star > 0) {
          break
        }
      }
      if (log(runif(1)) < min(0, (fun(alpha, beta) - fun(alpha_star, beta_star)) / t )) {
        alpha  = alpha_star
        beta = beta_star
        moving = moving + 1
      }
    }
    t = t * rho
  }
  return(list(a = alpha, b = beta))
}

# Annealing results
fun = function(a, b) {
  return(-1*log_f(a, b))
}
Ann_ret = annealing(fun)

print("For maximum likelihood estimates")
print(paste0("The MLE results are alpha: ", round(MLE_ret[1], 3), ", beta: ", round(MLE_ret[2], 3)))
print(paste0("The Annealing results are alpha: ", round(Ann_ret$a, 3), ", beta: ", round(Ann_ret$b, 3)))
```



## iii)

```{r}
# MLE result
log_f2 = function(a, b) {
  return(log_f(a, b) + log(a) - 2*a + log(b) - 2*b)
}
MLE_ret = MLE(log_f2)

# Annealing result
fun = function(a, b) {
  return(-1*log_f2(a, b))
}
Ann_ret = annealing(fun)
print("For maximum posterior estimates")
print(paste0("The MLE results are alpha: ", round(MLE_ret[1], 3), ", beta: ", round(MLE_ret[2], 3)))
print(paste0("The Annealing results are alpha: ", round(Ann_ret$a, 3), ", beta: ", round(Ann_ret$b, 3)))
```

## iv)

The results of posterior distribution are smaller than those of maximal likelihood distribution. The reason is that prior distribution is distributed close to 0. So the results would be more close to 0.


